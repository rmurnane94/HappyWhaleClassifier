{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b968d52-da1b-438f-99d1-a26fa13eb3f3",
   "metadata": {},
   "source": [
    "### ML Part 0: Introduction to Image analysis and Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc49516-d83b-4c31-8ab1-93d926c13059",
   "metadata": {},
   "source": [
    "In our machine learning assignment, we will be building a model that analyzes image data. We will familiarize ourselves with this data type in this jupyter notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f081730-edf5-444e-b8bf-7a0a7b724c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing packages \n",
    "import torch\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e94523-a02b-4f6b-823e-00e8c8354c8d",
   "metadata": {},
   "source": [
    "#### Importing images and applying a transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59e4851b-81a0-4430-8cea-b2e1c9a28d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ImageFolder\n",
      "    Number of datapoints: 9492\n",
      "    Root location: /projects/bgmp/shared/Bi625/ML_Assignment/Datasets/Whale_species/species_all\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "#specifying image file path\n",
    "images = \"/projects/bgmp/shared/Bi625/ML_Assignment/Datasets/Whale_species/species_all\"\n",
    "#specifying image transformation(s) - we are converting the image to a tensor which can be used by tensorflow\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "#loading in images and applying transformation\n",
    "all_images = datasets.ImageFolder(images, transform)\n",
    "print(all_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289ff67d-02e7-4469-aa1e-26e99607c918",
   "metadata": {},
   "source": [
    "#### Using python indexing, \"look\" at one of the images. What does is outputted? How is your image stored in all_images?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68413e6e-a2bb-4cf5-a24f-0ce2c88a1cd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ddd4a9a-c1b5-46a8-93a9-706fb6b51c8c",
   "metadata": {},
   "source": [
    "#### Using python indexing, what size is the image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c847bbcf-11e8-4bf9-bb97-8a883f583132",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c41698e-b5d6-4b81-9c16-251c7bfe2335",
   "metadata": {},
   "source": [
    "#### Look through the dataset, are all of the pictures the same size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b62408-23c5-4cad-a0ac-08d424a6dbc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15067c7b-2899-4f79-9b12-0e7358054da4",
   "metadata": {},
   "source": [
    "*Your notes here on the size of the images in the dataset*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334c13e0-00a9-4606-8211-6b6ed01072ea",
   "metadata": {},
   "source": [
    "#### Using matplotlib, \"look\" at the same image. How is this similar/different from our python indexing approach?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce607e8-39f9-4264-a618-52921331663b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4156b951-de3f-4c9d-af45-32db5859c822",
   "metadata": {},
   "source": [
    "*Your notes here on looking at the image with matplotlib*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3686fe-3aa1-490d-96ea-2516c0e88dab",
   "metadata": {},
   "source": [
    "#### Apply a new transformation to the data that changes the color of the image - check out options here: https://docs.pytorch.org/vision/0.11/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7f0f8e1-1084-4e95-be3a-d9dab5501ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ImageFolder\n",
      "    Number of datapoints: 9492\n",
      "    Root location: /projects/bgmp/shared/Bi625/ML_Assignment/Datasets/Whale_species/species_all\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "#specifying image file path\n",
    "images = \"/projects/bgmp/shared/Bi625/ML_Assignment/Datasets/Whale_species/species_all\"\n",
    "##### ADD YOUR TRANSFORMATION HERE #####\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "#loading in images and applying transformation\n",
    "all_images = datasets.ImageFolder(images, transform)\n",
    "print(all_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bf88b8-a125-42fb-8dfa-c1fba7bc5ed3",
   "metadata": {},
   "source": [
    "#### View the same image from above with BOTH python indexing and matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181165a4-c07d-4dcb-8594-6e840e09f0cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0661e8-118a-4b7a-97f5-2f6d79fa8230",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f472df-4641-48b0-b5d0-0728c03852eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c55e27a3-9c1c-4de9-99a8-4c7fc3e74f8c",
   "metadata": {},
   "source": [
    "#### Describe how your image changed with the transformation. Did the values change? Did the tensor/image size change? How does the image look?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e22bcb-de1b-4491-ba5b-914babfecb35",
   "metadata": {},
   "source": [
    "*Describe how your image changed with the transformation*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71b80b8-903c-44a6-9305-e2757f6e4a55",
   "metadata": {},
   "source": [
    "#### Apply a transformation to the data that crops the image - check out options here: https://docs.pytorch.org/vision/0.11/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0ee5d76-8b35-42c0-8d5f-790ec1b4c637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ImageFolder\n",
      "    Number of datapoints: 9492\n",
      "    Root location: /projects/bgmp/shared/Bi625/ML_Assignment/Datasets/Whale_species/species_all\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "#specifying image file path\n",
    "images = \"/projects/bgmp/shared/Bi625/ML_Assignment/Datasets/Whale_species/species_all\"\n",
    "##### ADD YOUR TRANSFORMATION HERE #####\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "#loading in images and applying transformation\n",
    "all_images = datasets.ImageFolder(images, transform)\n",
    "print(all_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538fbb71-0c61-4520-92f3-396423846b3d",
   "metadata": {},
   "source": [
    "#### View the same image from above with BOTH python indexing and matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64d4513-94b0-4187-b12b-d236eecde47a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d2ff57-befa-4dc0-ac28-8146a475bc19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2df5c7b-1e4b-4e96-a4eb-cab5a6acdc3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8840170-0657-4ee6-a03b-ed8c16f85d7c",
   "metadata": {},
   "source": [
    "#### Describe how your image changed with the transformation. Did the values change? Did the tensor/image size change? How does the image look?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e327bfe9-206d-4145-a077-6eae3d502822",
   "metadata": {},
   "source": [
    "*Describe how your image changed with the transformation*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0481191-7fb6-436f-a14f-bf88bafea906",
   "metadata": {},
   "source": [
    "#### Look through the dataset, are all of the pictures the same size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc887b24-2925-4c66-ba32-352709c00f6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7c8975d-17f1-4e4b-bbe5-ebdd8fa341ec",
   "metadata": {},
   "source": [
    "*Describe whether the image sizes are now the same*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4df8794-2aab-4abd-bd2b-2ef506678c77",
   "metadata": {},
   "source": [
    "#### Determine an image transformation approach that would give you optimal input images for a machine learning whale species classifier\n",
    "\n",
    "You want: \n",
    "- All images converted to numerics\n",
    "- All images to be the same size\n",
    "- A data normalization approach\n",
    "- Data augmentation\n",
    "\n",
    "Make sure you visualize a few images to make sure your chosen approach was successful!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01cd0e80-a416-4efe-89dd-d68566475a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ImageFolder\n",
      "    Number of datapoints: 9492\n",
      "    Root location: /projects/bgmp/shared/Bi625/ML_Assignment/Datasets/Whale_species/species_all\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "#specifying image file path\n",
    "images = \"/projects/bgmp/shared/Bi625/ML_Assignment/Datasets/Whale_species/species_all\"\n",
    "##### ADD YOUR TRANSFORMATION HERE #####\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "#loading in images and applying transformation\n",
    "all_images = datasets.ImageFolder(images, transform)\n",
    "print(all_images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
